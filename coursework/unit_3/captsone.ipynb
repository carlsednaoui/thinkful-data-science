{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting when a company will send it's next marketing email\n",
    "\n",
    "This project aims to predict when a company will send it's next marketing email. The data comes from [MailCharts](https://www.mailcharts.com/).\n",
    "\n",
    "Predicting when your competitors will send their next marketing blast allows you to get ahead of the game and send your email before they do.\n",
    "\n",
    "- Sample data: http://share.mailcharts.com/0b141Z3c1M40\n",
    "- All data: _coming soon_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries and dependencies\n",
    "\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from xgboost import XGBRegressor, plot_tree\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39864, 6)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "\n",
    "df = pd.read_csv('./data/2017-12-capstone.csv')\n",
    "# df = pd.read_csv('./data/capstone-sample.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add first set of features we need\n",
    "\n",
    "df['sent_time'] = pd.to_datetime(df.sent_at)\n",
    "df['hour'] = df.sent_time.apply(lambda x: x.hour)\n",
    "df['day'] = df.sent_time.apply(lambda x: x.day)\n",
    "df['month'] = df.sent_time.apply(lambda x: x.month)\n",
    "df['year'] = df.sent_time.apply(lambda x: x.year)\n",
    "df['day_of_week'] = df.sent_time.dt.weekday_name \n",
    "df['day_of_week_num'] = df.sent_time.dt.weekday \n",
    "df['weekend'] = df.day_of_week.apply(lambda x: 1 if x=='Saturday' or x=='Sunday' else  0) \n",
    "df[\"weeknum\"] = df.sent_time.dt.weekofyear\n",
    "df[\"am_pm\"] = df.sent_time.apply(lambda x: 1 if x.strftime('%p') == \"AM\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(\"sent_at\", ascending=True)\n",
    "\n",
    "results = []\n",
    "companies = df[\"company_id\"].unique()\n",
    "\n",
    "for c in companies:\n",
    "    company_emails = df[df[\"company_id\"] == c]\n",
    "    previous_email_sent_at = None\n",
    "\n",
    "    for index, row in company_emails.iterrows():\n",
    "        if previous_email_sent_at == None:    \n",
    "            previous_email_sent_at = row[\"sent_at\"]\n",
    "            df.loc[index, \"prev_email\"] = 0\n",
    "        else:\n",
    "            df.loc[index, \"prev_email\"] = previous_email_sent_at\n",
    "            previous_email_sent_at = row[\"sent_at\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the time delta from when the previous email was sent\n",
    "\n",
    "def construct_full_date(timestamp):\n",
    "    # 2017-01-01 00:01:32\n",
    "    # format: year, month, day, hour, minute, seconds\n",
    "    date = re.split(\" |\\-|\\:\", timestamp)\n",
    "    _date = [int(x) for x in date]\n",
    "    return datetime(year=_date[0], month=_date[1], day=_date[2], hour=_date[3], minute=_date[4], second=_date[5])\n",
    "\n",
    "def get_time_delta(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (construct_full_date(a) - construct_full_date(b)).total_seconds()\n",
    "    \n",
    "df[\"delta_in_seconds\"] = df.apply(lambda x: get_time_delta(x[\"sent_at\"], x[\"prev_email\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QA that we only have 1 time_delta with a value of 0\n",
    "\n",
    "df[df[\"delta_in_seconds\"] == 0][\"company_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a few more features which will make it easier to read our time deltas\n",
    "\n",
    "df[\"delta_in_hours\"] = df[\"delta_in_seconds\"] / 60 / 60\n",
    "df[\"delta_in_days\"] = df[\"delta_in_seconds\"] / 60 / 60 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get company dummies. This will help us calculate each prediction at a company level\n",
    "\n",
    "dummy_company = pd.get_dummies(df[\"company_name\"])\n",
    "dummy_company.head()\n",
    "\n",
    "# Merge both dataframes\n",
    "\n",
    "df = pd.merge(df, dummy_company, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_df = df.loc[df[\"company_name\"].isin([\"JackThreads\", \"Patagonia\", \"Bloomingdale's\", \"Target\", \"Segment\", \"Williams Sonoma\", \"1-800-Flowers.com\", \"Seamless\", \"Kidbox\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize  send volume to understand the data a bit better\n",
    "# Takeaway #1: Email volume varies drastically by company.\n",
    "\n",
    "email_counts_sum = sub_df.groupby([\"company_name\"])[\"delta_in_seconds\"].count()\n",
    "bars = email_counts_sum.plot(kind='bar')\n",
    "bars.set_ylabel(\"Email volume\")\n",
    "bars.set_xlabel(\"Company name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's analyze the sending behavior of a few companies\n",
    "# Takeaway #2: Every company has their own sending behavior throughout the year!\n",
    "# Takeaway #3: Seasonality is likely to impact email frequency.\n",
    "# Note: Spikes == more time in between emails\n",
    "\n",
    "time_delta_average_by_month = sub_df.groupby(by=['company_name', 'month'])[\"delta_in_seconds\"].mean().reset_index()\n",
    "c_name = time_delta_average_by_month[\"company_name\"].unique()\n",
    "\n",
    "for i in c_name:\n",
    "    company = time_delta_average_by_month[time_delta_average_by_month[\"company_name\"] == i].reset_index()\n",
    "    plot = company.plot(x=\"month\", y=\"delta_in_seconds\")\n",
    "    plot.set_xlabel(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It's not 100% clear here, but it looks like industry could be a helpful feature to include in our model\n",
    "# Let's visualize this\n",
    "\n",
    "sub_df.groupby([\"industry_name\"]).delta_in_seconds.mean().astype(int).reset_index().sort_values(\"delta_in_seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the send volume to understand the data a bit better\n",
    "# Takeaway #4: Consider adding industry as a feature\n",
    "\n",
    "f = plt.figure(1, figsize = (15,12))\n",
    "email_between_time_avg = df.groupby([\"industry_name\"]).delta_in_seconds.mean()\n",
    "email_between_time_avg.plot(kind='bar')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's analyze all of the data by month to have a better look at seasonality\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel('Time between two emails')\n",
    "\n",
    "df.groupby(\"month\").delta_in_seconds.mean().plot(color='blue', label='2016')\n",
    "\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "plt.legend(h1, l1, loc=2)\n",
    "plt.show()\n",
    "\n",
    "# We can see the impact of seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, let's look at the distribution of our email delta\n",
    "\n",
    "sns.boxplot(x=df[\"delta_in_seconds\"])\n",
    "\n",
    "# Look at that...! We have some outliers. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove prev_time_delta outliers\n",
    "\n",
    "stdev = df.delta_in_seconds.std()\n",
    "mean = df.delta_in_seconds.mean()\n",
    "upper_bound = mean + (stdev * 3)\n",
    "lower_bound = mean - (stdev * 3)\n",
    "\n",
    "non_outliers = df[(df.delta_in_seconds >= lower_bound) & (df.delta_in_seconds <= upper_bound)]\n",
    "\n",
    "print(df.shape)\n",
    "print(non_outliers.shape)\n",
    "print(\"We removed\", df.shape[0] - non_outliers.shape[0], \"outliers\")\n",
    "sns.boxplot(x=non_outliers[\"delta_in_seconds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This looks good, let's assign this back to our df variable\n",
    "\n",
    "df = non_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get a graps of how our data is distributed\n",
    "\n",
    "df.delta_in_hours.hist(bins = 50)\n",
    "\n",
    "# Look at that, it's left skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's build a heatmap to better understand the impact of the day of the week.\n",
    "\n",
    "f = plt.figure(1, figsize = (15,12))\n",
    "pvt = pd.pivot_table(data=sub_df,values='delta_in_days',index='day_of_week_num', columns='company_name')\n",
    "sns.heatmap(pvt, annot=True)\n",
    "\n",
    "plt.title('Average Time Between Emails',fontsize=25)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel('company_name',fontsize=18)\n",
    "plt.ylabel('Day of Week',fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_of_interest = ['id', 'company_name', 'company_id', 'subject', 'sent_at', 'industry_name', \"sent_time\", \"hour\", \"day\", 'month', 'day_of_week', 'day_of_week_num', 'weekend', 'weeknum', 'am_pm', 'delta_in_hours', 'delta_in_days', 'delta_in_seconds']\n",
    "\n",
    "f = plt.figure(1, figsize = (15,12))\n",
    "sns.heatmap(sub_df[features_of_interest].corr())\n",
    "plt.show()\n",
    "\n",
    "# day_of_week_num and weekend are correlated, let's not include both of these when building our models\n",
    "# same with delta_in_<timeframe> — let's only pick one of the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's analyze seasonality\n",
    "\n",
    "def season_map(month):\n",
    "    season=''\n",
    "    if month <=3:\n",
    "        season='Winter'\n",
    "    elif month<=6:\n",
    "        season='Spring'\n",
    "    elif month<=9:\n",
    "        season='Summer'\n",
    "    else: \n",
    "        season='Fall'\n",
    "    return season\n",
    "\n",
    "df['season'] = df.month.apply(lambda x:season_map(x))\n",
    "df['season_num'] = pd.Categorical(df.season).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There is no big difference across the different seasons\n",
    "\n",
    "f = plt.figure(1, figsize = (15,12))\n",
    "sns.violinplot(x='season', y='delta_in_seconds', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done with EDA. The modeling begins!\n",
    "\n",
    "Features we want:\n",
    "\n",
    "- Month\n",
    "- Company\n",
    "- Industry(?)\n",
    "- Season? (Month can help with this though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for c in df.columns:\n",
    "#    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert industry name to codes\n",
    "df['industry_num'] = pd.Categorical(df.industry_name).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = df.delta_in_seconds\n",
    "companies = df[\"company_name\"].unique()\n",
    "features_of_interest = np.append([companies], ['month', 'industry_num'])\n",
    "features = df[features_of_interest]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=100)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Naive prediction: Our benchmark\n",
    "# TODO: Could we get a benchmark without using any models?\n",
    "# Get a company's average time_delta for the training group. Compare it against the expected values for the test group.\n",
    "# But now that I think about it we need some form of model to run this through...!\n",
    "\n",
    "# benchmark_company = \"Segment\"\n",
    "# benchmark_mean = df[df[benchmark_company] == 1].delta_in_hours.mean()\n",
    "\n",
    "# abs(y_test[X_test[benchmark_company]==1 ]- gbr.predict(X_test[X_test[benchmark_company]==1])).mean()\n",
    "# abs(y_test[X_test[benchmark_company]==1 ]-  benchmark_mean).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize our fit our model, Linear Regression.\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_pred = lr.predict(X_test)\n",
    "lr.score(X_test, y_test)\n",
    "\n",
    "# 50 is not that great. Let's see what we can do to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at a decision tree to see if we can undestand how our data gets classified\n",
    "\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "decision_tree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    decision_tree,\n",
    "    out_file=None,\n",
    "    feature_names=features_of_interest,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(decision_tree.score(X_test, y_test))\n",
    "# These results are really bad. oy! There's not much we can learn from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perhaps a random using many trees can help improve the outcome? Let's try a random forrest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=3, random_state=100)\n",
    "regr.fit(X_train, y_train)\n",
    "print(regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can get the score up to ~45, but now we risk overfitting with such a high depth\n",
    "regr = RandomForestRegressor(max_depth=50, random_state=100)\n",
    "regr.fit(X_train, y_train)\n",
    "print(regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try some fancier models\n",
    "\n",
    "xgr=XGBRegressor()\n",
    "xgr.fit(X_train, y_train) \n",
    "xgr.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adr = AdaBoostRegressor()\n",
    "adr.fit(X_train, y_train)\n",
    "adr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train) \n",
    "gbr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's revise our feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New features\n",
    "\n",
    "df['subject_promo']=df.subject.str.contains('\\%|\\$|free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = df.delta_in_seconds\n",
    "companies = df[\"company_name\"].unique()\n",
    "features_of_interest = np.append([companies], ['month', 'industry_num', 'am_pm', 'day_of_week_num', 'subject_promo'])\n",
    "features = df[features_of_interest]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=100)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgr.fit(X_train, y_train) \n",
    "xgr.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adr.fit(X_train, y_train)\n",
    "adr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr.fit(X_train, y_train) \n",
    "gbr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on average email in between time by company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's use GradientBoosting since it had the highest score above\n",
    "gbr=GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)\n",
    "gbr.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companies = df[\"company_name\"].unique()\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_vs_benchmark(company):\n",
    "    benchmark_mean = df[df[company] == 1].delta_in_hours.mean()\n",
    "    # Our model error\n",
    "    model_error = abs(y_test[X_test[company]==1 ]- gbr.predict(X_test[X_test[company]==1])).mean()\n",
    "    # The benchmark error\n",
    "    benchmark_error = abs(y_test[X_test[company]==1 ]-  benchmark_mean).mean()\n",
    "    improvement = benchmark_error / model_error\n",
    "    print(\"Model error: \", np.floor(model_error))\n",
    "    print(\"Benchmark error: \", np.floor(benchmark_error))\n",
    "    print(\"Improvement: \", np.round(improvement, 2), \"x\")\n",
    "    return improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in companies:\n",
    "    print(\"**\", c, \"**\")\n",
    "    model_vs_benchmark(c)\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
