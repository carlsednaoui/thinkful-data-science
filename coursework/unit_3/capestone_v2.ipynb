{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries and dependencies\n",
    "\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4666, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "# http://share.mailcharts.com/3K3e1d2W1h27\n",
    "\n",
    "df = pd.read_csv('./data/capstone2-v2.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "\n",
    "# Turn everything to lowercase\n",
    "df[\"subject\"] = df[\"subject\"].str.lower()\n",
    "df[\"full_text\"] = df[\"full_text\"].str.lower()\n",
    "\n",
    "# Remove non alphanumeric characters\n",
    "df[\"subject\"] = df[\"subject\"].replace(\"[^A-Za-z0-9]+\", \" \", regex=True)\n",
    "df[\"full_text\"] = df[\"full_text\"].replace(\"[^A-Za-z0-9]+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure our cart_abandon column is a number\n",
    "df[\"cart_abandon\"] = pd.to_numeric(df[\"cart_abandon\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing data (unclassified emails)\n",
    "df = df.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have this many rows:  1581\n",
      "With this many identified cart abandon emails:  264.0\n",
      "Percent of cart abandon emails:  0.16698292220113853\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg_id</th>\n",
       "      <th>add_id</th>\n",
       "      <th>email_guid</th>\n",
       "      <th>sent_at</th>\n",
       "      <th>subject</th>\n",
       "      <th>full_text</th>\n",
       "      <th>r</th>\n",
       "      <th>email_url</th>\n",
       "      <th>cart_abandon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2582</td>\n",
       "      <td>3742</td>\n",
       "      <td>f3870de1-3ab6-3fed-3fe2-778a74f3197e</td>\n",
       "      <td>1/7/16 15:07</td>\n",
       "      <td>welcome to sephora beauty insider</td>\n",
       "      <td>lorem you re a beauty insider web version seph...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.mailcharts.com/emails/f3870de1-3ab...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2582</td>\n",
       "      <td>3742</td>\n",
       "      <td>0880fd5c-fbc5-eeb2-5bd3-8e352eae2b70</td>\n",
       "      <td>1/8/16 17:28</td>\n",
       "      <td>new year new rewards</td>\n",
       "      <td>lorem the january rewards are here web version...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.mailcharts.com/emails/0880fd5c-fbc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2582</td>\n",
       "      <td>3742</td>\n",
       "      <td>db726d24-2477-ccd6-a0aa-902dcf07f4b9</td>\n",
       "      <td>1/8/16 18:54</td>\n",
       "      <td>a friendly reminder</td>\n",
       "      <td>lorem your product picks are waiting at checko...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.mailcharts.com/emails/db726d24-247...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reg_id  add_id                            email_guid       sent_at  \\\n",
       "0    2582    3742  f3870de1-3ab6-3fed-3fe2-778a74f3197e  1/7/16 15:07   \n",
       "1    2582    3742  0880fd5c-fbc5-eeb2-5bd3-8e352eae2b70  1/8/16 17:28   \n",
       "2    2582    3742  db726d24-2477-ccd6-a0aa-902dcf07f4b9  1/8/16 18:54   \n",
       "\n",
       "                             subject  \\\n",
       "0  welcome to sephora beauty insider   \n",
       "1               new year new rewards   \n",
       "2               a friendly reminder    \n",
       "\n",
       "                                           full_text  r  \\\n",
       "0  lorem you re a beauty insider web version seph...  1   \n",
       "1  lorem the january rewards are here web version...  2   \n",
       "2  lorem your product picks are waiting at checko...  3   \n",
       "\n",
       "                                           email_url  cart_abandon  \n",
       "0  https://www.mailcharts.com/emails/f3870de1-3ab...           0.0  \n",
       "1  https://www.mailcharts.com/emails/0880fd5c-fbc...           0.0  \n",
       "2  https://www.mailcharts.com/emails/db726d24-247...           1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"We have this many rows: \", df.shape[0])\n",
    "print(\"With this many identified cart abandon emails: \", df[\"cart_abandon\"].sum())\n",
    "print(\"Percent of cart abandon emails: \", df[\"cart_abandon\"].sum() / df.shape[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DF with all our cart abandon emails\n",
    "ca = df[df[\"cart_abandon\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your         144\n",
       "you           92\n",
       "order         52\n",
       "complete      46\n",
       "something     38\n",
       "cart          37\n",
       "in            36\n",
       "purchase      29\n",
       "we            29\n",
       "items         27\n",
       "left          26\n",
       "forget        26\n",
       "did           25\n",
       "for           25\n",
       "off           23\n",
       "t             23\n",
       "to            23\n",
       "don           19\n",
       "still         19\n",
       "at            18\n",
       "lorem         17\n",
       "shopping      15\n",
       "s             15\n",
       "is            15\n",
       "10            14\n",
       "a             14\n",
       "back          13\n",
       "free          13\n",
       "ve            11\n",
       "have          11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the most popular words in the subject for cart abandon emails\n",
    "pd.Series(\" \".join(ca[\"subject\"]).lower().split()).value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to          1327\n",
       "your        1249\n",
       "you         1160\n",
       "the          707\n",
       "and          549\n",
       "we           523\n",
       "in           456\n",
       "email        443\n",
       "us           441\n",
       "for          440\n",
       "order        427\n",
       "a            395\n",
       "on           383\n",
       "this         374\n",
       "our          356\n",
       "com          340\n",
       "or           330\n",
       "complete     314\n",
       "if           314\n",
       "s            306\n",
       "cart         285\n",
       "have         282\n",
       "here         269\n",
       "of           255\n",
       "with         240\n",
       "click        239\n",
       "at           231\n",
       "1            229\n",
       "free         228\n",
       "t            228\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the most popular words in the full_text for cart abandon emails\n",
    "pd.Series(\" \".join(ca[\"full_text\"]).lower().split()).value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's hand-pick a few words and see if we can build a classifier this way\n",
    "hand_picked_words_positive = [\"complete\", \"forget\", \"forgot\", \"left\", \"cart\", \"items\", \"still\", \"something\", \"saved\", \"order\", \"waiting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's add a new column to DF for each of the hand-picked words looking at both the subject or the full_text\n",
    "# To keep our data clean, we'll create a copy of DF\n",
    "\n",
    "df1 = df.copy()\n",
    "for w in hand_picked_words_positive:\n",
    "    df1[w] = df1[\"subject\"].str.contains(w) | df1[\"full_text\"].str.contains(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg_id</th>\n",
       "      <th>add_id</th>\n",
       "      <th>email_guid</th>\n",
       "      <th>sent_at</th>\n",
       "      <th>subject</th>\n",
       "      <th>full_text</th>\n",
       "      <th>r</th>\n",
       "      <th>email_url</th>\n",
       "      <th>cart_abandon</th>\n",
       "      <th>complete</th>\n",
       "      <th>forget</th>\n",
       "      <th>forgot</th>\n",
       "      <th>left</th>\n",
       "      <th>cart</th>\n",
       "      <th>items</th>\n",
       "      <th>still</th>\n",
       "      <th>something</th>\n",
       "      <th>saved</th>\n",
       "      <th>order</th>\n",
       "      <th>waiting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2582</td>\n",
       "      <td>3742</td>\n",
       "      <td>f3870de1-3ab6-3fed-3fe2-778a74f3197e</td>\n",
       "      <td>1/7/16 15:07</td>\n",
       "      <td>welcome to sephora beauty insider</td>\n",
       "      <td>lorem you re a beauty insider web version seph...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.mailcharts.com/emails/f3870de1-3ab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2582</td>\n",
       "      <td>3742</td>\n",
       "      <td>0880fd5c-fbc5-eeb2-5bd3-8e352eae2b70</td>\n",
       "      <td>1/8/16 17:28</td>\n",
       "      <td>new year new rewards</td>\n",
       "      <td>lorem the january rewards are here web version...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.mailcharts.com/emails/0880fd5c-fbc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2582</td>\n",
       "      <td>3742</td>\n",
       "      <td>db726d24-2477-ccd6-a0aa-902dcf07f4b9</td>\n",
       "      <td>1/8/16 18:54</td>\n",
       "      <td>a friendly reminder</td>\n",
       "      <td>lorem your product picks are waiting at checko...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.mailcharts.com/emails/db726d24-247...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reg_id  add_id                            email_guid       sent_at  \\\n",
       "0    2582    3742  f3870de1-3ab6-3fed-3fe2-778a74f3197e  1/7/16 15:07   \n",
       "1    2582    3742  0880fd5c-fbc5-eeb2-5bd3-8e352eae2b70  1/8/16 17:28   \n",
       "2    2582    3742  db726d24-2477-ccd6-a0aa-902dcf07f4b9  1/8/16 18:54   \n",
       "\n",
       "                             subject  \\\n",
       "0  welcome to sephora beauty insider   \n",
       "1               new year new rewards   \n",
       "2               a friendly reminder    \n",
       "\n",
       "                                           full_text  r  \\\n",
       "0  lorem you re a beauty insider web version seph...  1   \n",
       "1  lorem the january rewards are here web version...  2   \n",
       "2  lorem your product picks are waiting at checko...  3   \n",
       "\n",
       "                                           email_url  cart_abandon complete  \\\n",
       "0  https://www.mailcharts.com/emails/f3870de1-3ab...           0.0    False   \n",
       "1  https://www.mailcharts.com/emails/0880fd5c-fbc...           0.0    False   \n",
       "2  https://www.mailcharts.com/emails/db726d24-247...           1.0    False   \n",
       "\n",
       "  forget forgot   left   cart  items  still something  saved  order waiting  \n",
       "0  False  False  False  False  False  False     False  False  False   False  \n",
       "1  False  False  False  False  False  False     False  False   True   False  \n",
       "2  False  False  False  False   True  False     False  False   True    True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the features (x) and target (y) to build our models\n",
    "X = df1[hand_picked_words_positive]\n",
    "y = df1.cart_abandon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create our train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93181818181818177"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin by using logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "\n",
    "# Not a bad score! We know that the majority of our data is non cart abandonment emails.\n",
    "# Nonetheless, this performs better than a random guess would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[332,   5],\n",
       "       [ 22,  37]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time to build a confusion matrix\n",
    "\n",
    "# 332 = it is class 0, we predicted class 0\n",
    "# 5 = it is class 0, we predicted 1\n",
    "# 23 = it is class 1, we predicted class 0\n",
    "# 36 = it is class 1, we predicted class 1\n",
    "\n",
    "from  sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, lr.predict(X_test))\n",
    "\n",
    "# There are some improvements to be made when it comes to detecting cart abandon emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 15708)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if we get a better result if we were to instead vectorize each word in our subjects.\n",
    "# This helps us understand how important a word is.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Combine full_text and subject so that we can vectorize it\n",
    "df1[\"subject_and_text\"] = df[\"subject\"] + \" \" + df[\"full_text\"]\n",
    "\n",
    "subject_vector = vectorizer.fit_transform(df1[\"subject_and_text\"])\n",
    "subject_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the features (x) and target (y) to build our models\n",
    "X = subject_vector\n",
    "# y remains the same\n",
    "\n",
    "# Create our train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90656565656565657"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The score is slightly higher\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337,   0],\n",
       "       [ 37,  22]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the confusion matrix\n",
    "confusion_matrix(y_test, lr.predict(X_test))\n",
    "\n",
    "# This model actually performs slightly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 15708)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try another approach, with CountVectorizer — this will tell us how often a word occurs\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "subject_vector = vectorizer.fit_transform(df1[\"subject_and_text\"])\n",
    "subject_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the features (x) and target (y) to build our models\n",
    "X = subject_vector\n",
    "# y remains the same\n",
    "\n",
    "# Create our train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95707070707070707"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[329,   8],\n",
       "       [  9,  50]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model performs better. We have a few false positives, but we can likely live with that.\n",
    "confusion_matrix(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88636363636363635"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's give Gausian NB a whirl\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "\n",
    "# We need to convert the sparse matrix to an array\n",
    "X = subject_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Womp womp. That's our worst model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maybe looking at 1 word is not enough.\n",
    "# Let's try using bigrams and trigrams.\n",
    "\n",
    "from nltk import ngrams\n",
    "n = 2\n",
    "# We'll use a set to make sure our ngrams are unique\n",
    "subject_bigrams = set()\n",
    "full_text_bigrams = set()\n",
    "\n",
    "for i, r in df.iterrows():\n",
    "    subject_grams = ngrams(r[\"subject\"].split(), n)\n",
    "    row_grams = []\n",
    "    for sg in subject_grams:\n",
    "#         subject_bigrams.add(\"_\".join(sg))\n",
    "        subject_bigrams.add(sg)\n",
    "        row_grams.append(sg)\n",
    "\n",
    "    full_text_grams = ngrams(r[\"full_text\"].split(), n)\n",
    "    for ftg in full_text_grams:\n",
    "        full_text_bigrams.add(ftg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "\n",
    "n = 3\n",
    "subject_trigrams = set()\n",
    "full_text_trigrams = set()\n",
    "\n",
    "for i, r in df.iterrows():\n",
    "    subject_grams = ngrams(r[\"subject\"].split(), n)\n",
    "    for sg in subject_grams:\n",
    "        subject_trigrams.add(sg)\n",
    "    \n",
    "    full_text_grams = ngrams(r[\"full_text\"].split(), n)\n",
    "    for ftg in full_text_grams:\n",
    "        full_text_trigrams.add(ftg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = subject_bigrams.union(full_text_bigrams)\n",
    "trigrams = subject_trigrams.union(full_text_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94147"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_unigram_sentence(sentence):\n",
    "#     return [word for word in word_tokenize(sentence.lower()) if word not in stopwords_set and word not in punctuation]\n",
    "\n",
    "# Function to get unigrams of question1 and question2.\n",
    "# def get_unigrams(df):\n",
    "#     df['question1_unigram'] = df['question1'].apply(lambda x: get_unigram_sentence(x.decode(encoding='utf-8')))\n",
    "#     df['question2_unigram'] = df['question2'].apply(lambda x: get_unigram_sentence(x.decode(encoding='utf-8')))\n",
    "    \n",
    "# def get_bigrams(df):\n",
    "#     df['subject_bigrams'] = df['question1_unigram'].apply(lambda x: [i for i in ngrams(x, 2)])\n",
    "#     df['full_text_bigram'] = df['question2_unigram'].apply(lambda x: [i for i in ngrams(x, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 108951)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "# Combine full_text and subject so that we can vectorize it\n",
    "df2[\"subject_and_text\"] = df2[\"subject\"] + \" \" + df2[\"full_text\"]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "subject_vector = vectorizer.fit_transform(df2[\"subject_and_text\"])\n",
    "subject_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95959595959595956"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features (x) and target (y) to build our models\n",
    "X = subject_vector\n",
    "y = df2[\"cart_abandon\"]\n",
    "\n",
    "# Create our train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[332,   5],\n",
       "       [ 11,  48]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 108951)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "# Combine full_text and subject so that we can vectorize it\n",
    "df2[\"subject_and_text\"] = df2[\"subject\"] + \" \" + df2[\"full_text\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "subject_vector = vectorizer.fit_transform(df2[\"subject_and_text\"])\n",
    "subject_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88888888888888884"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features (x) and target (y) to build our models\n",
    "X = subject_vector\n",
    "y = df2[\"cart_abandon\"]\n",
    "\n",
    "# Create our train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337,   0],\n",
       "       [ 44,  15]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne_model = TSNE(n_components=2, verbose=1, random_state=0, learning_rate=100)\n",
    "# tsne_tfidf = tsne_model.fit_transform(subject_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
