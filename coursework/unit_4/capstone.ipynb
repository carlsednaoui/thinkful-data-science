{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone\n",
    "\n",
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab the data from here: http://share.mailcharts.com/0D0Q2e0L1s47 and http://share.mailcharts.com/0z0F3m1X0l39\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3346, 8)\n",
      "Index(['reg_id', 'add_id', 'name', 'email_guid', 'sent_at', 'subject',\n",
      "       'full_text', 'r'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/capstone-v2.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "df = df.drop_duplicates(\"email_guid\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Steps: Clean up text, stemming, remove stop words and weird chars, tokenizer words\n",
    "\n",
    "# punctuation = list(set(string.punctuation))\n",
    "re_punctuation = \"\\.|\\>|\\/|\\)|\\\"|\\(|\\}|\\'|\\_|\\-|\\$|\\:|\\[|\\^|\\+|\\?|\\`|\\~|\\!|\\<|\\@|\\;|\\=|\\*|\\\\\\|\\{|\\&|\\]|\\||\\,|\\|\"\n",
    "stopwords_set = list(set(stopwords.words('english')))\n",
    "handpicked_works = [\"com\"]\n",
    "\n",
    "def get_unigram_sentence(sentence, company_name):\n",
    "    company_names = company_name.lower().split(\" \")\n",
    "    company_names.append(company_name.lower().replace(\" \", \"\"))\n",
    "    \n",
    "    sentence_no_punc = re.sub(re_punctuation, \" \", sentence)\n",
    "    unigram = [word for word in word_tokenize(sentence_no_punc.lower()) if word not in stopwords_set and word not in company_names and word not in handpicked_works]\n",
    "    return unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "\n",
    "for i, el in df.iterrows():\n",
    "    tokenized_text.append(get_unigram_sentence(el['subject'], el['name']))\n",
    "\n",
    "df[\"tokenized_text\"] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_stems(words):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"stemmed_tokens\"] = df.tokenized_text.apply(lambda x: get_stems(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"stemmed_text\"] = df[\"stemmed_tokens\"].apply(lambda x: \" \".join(word for word in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=50, ngram_range=(1,2))\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=50)\n",
    "\n",
    "vectorizer = vectorizer.fit(df[\"stemmed_text\"])\n",
    "X = vectorizer.transform(df[\"stemmed_text\"])\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Split our data in test / train\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test = train_test_split(X, test_size=0.25, random_state=100)\n",
    "# df_train, df_test = train_test_split(df, test_size=0.25, random_state=100)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carl/anaconda/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=6, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=22,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "vectorized_values = X\n",
    "\n",
    "lda = LDA(6, random_state=22)\n",
    "lda.fit(vectorized_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('order', 445.08218459728533), ('confirm', 252.6752500801897), ('way', 86.306978332987455), ('account', 86.292800234261648), ('deliveri', 48.929519823662744), ('shipment', 0.16885467852286359), ('lorem', 0.16754661553781106), ('shop', 0.16710712963970814), ('ship', 0.1670784648325786), ('10', 0.16693710413664875)]\n",
      "====================================================================================================\n",
      "Topic 1:\n",
      "[('thank', 202.57543048581752), ('save', 126.27201437301225), ('today', 98.013599796547169), ('gift', 95.983738739454353), ('order', 88.838738939998265), ('holiday', 67.478880090831652), ('shop', 0.18557031272709137), ('sale', 0.17409874360560051), ('day', 0.16817820461404986), ('purchas', 0.16783119077827086)]\n",
      "====================================================================================================\n",
      "Topic 2:\n",
      "[('ship', 312.02446466197), ('welcom', 303.88010437450714), ('order', 173.52711027121816), ('free', 160.58686268614045), ('lorem', 109.47121944802758), ('shipment', 52.260908901516721), ('10', 0.16749192656029716), ('way', 0.16736303146239262), ('account', 0.16726959559107013), ('today', 0.16726251931626868)]\n",
      "====================================================================================================\n",
      "Topic 3:\n",
      "[('sale', 153.32583085393173), ('end', 86.740237960142508), ('30', 79.483781217699885), ('purchas', 71.730792815507897), ('style', 70.505369737787561), ('extra', 66.440688972435538), ('25', 59.940276271848482), ('15', 53.300033469613588), ('today', 0.16808552956283651), ('save', 0.16762910062038597)]\n",
      "====================================================================================================\n",
      "Topic 4:\n",
      "[('new', 221.70574350345777), ('order', 80.071655943828446), ('10', 70.660900591789755), ('receiv', 49.543324500250641), ('account', 0.16722287911910041), ('save', 0.16716067729096187), ('30', 0.1671076572045268), ('lorem', 0.16707200331457248), ('free', 0.16702001582824905), ('deal', 0.16701238563251766)]\n",
      "====================================================================================================\n",
      "Topic 5:\n",
      "[('day', 131.0759526413969), ('50', 109.69602120544462), ('shop', 92.268998456076815), ('deal', 83.644437037144684), ('20', 83.50819777321037), ('start', 61.265661616678564), ('extra', 17.192681794020114), ('way', 0.17630837769423707), ('holiday', 0.16842062938718991), ('sale', 0.16832045280186558)]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        labels.append([(vectorizer.get_feature_names()[i]) for i in topic.argsort()[:-1-1:-1]][0])\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "print_topics(lda, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, let's assign this to our original text\n",
    "\n",
    "vectorized_values_lda = lda.transform(vectorized_values)\n",
    "\n",
    "predicted_label = []\n",
    "for i in vectorized_values_lda:\n",
    "    # Get the highest value. We'll consider that to be the predicted label.\n",
    "    predicted_label.append(labels[i.argsort()[-1]])\n",
    "\n",
    "df[\"lda_predicted_label\"] = predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg_id</th>\n",
       "      <th>add_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email_guid</th>\n",
       "      <th>sent_at</th>\n",
       "      <th>subject</th>\n",
       "      <th>full_text</th>\n",
       "      <th>r</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lda_predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6361</td>\n",
       "      <td>7526</td>\n",
       "      <td>Le Creuset</td>\n",
       "      <td>45f2d9ed-128e-9ae5-b8f1-4e224a02dfca</td>\n",
       "      <td>2017-01-10 21:34:33</td>\n",
       "      <td>Welcome, Lorem Ipsum!</td>\n",
       "      <td>LE CREUSET Welcome to Le Creuset. To log in wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>[welcome, lorem, ipsum]</td>\n",
       "      <td>[welcom, lorem, ipsum]</td>\n",
       "      <td>welcom lorem ipsum</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6361</td>\n",
       "      <td>7526</td>\n",
       "      <td>Le Creuset</td>\n",
       "      <td>34db5cee-2a9c-17f4-b97d-68343ad26f19</td>\n",
       "      <td>2017-01-10 21:36:48</td>\n",
       "      <td>Hi! You were looking for free shipping, right?</td>\n",
       "      <td>Save a bundle on shipping with code LECREUSETL...</td>\n",
       "      <td>2</td>\n",
       "      <td>[hi, looking, free, shipping, right]</td>\n",
       "      <td>[hi, look, free, ship, right]</td>\n",
       "      <td>hi look free ship right</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6361</td>\n",
       "      <td>7526</td>\n",
       "      <td>Le Creuset</td>\n",
       "      <td>55f96ec8-739f-4a3a-63c4-ec1fddcf795d</td>\n",
       "      <td>2017-01-10 21:41:43</td>\n",
       "      <td>Le Creuset: New Order # 200068673</td>\n",
       "      <td>LE CREUSET Thank you for your order from Le Cr...</td>\n",
       "      <td>3</td>\n",
       "      <td>[new, order, #, 200068673]</td>\n",
       "      <td>[new, order, #, 200068673]</td>\n",
       "      <td>new order # 200068673</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6361</td>\n",
       "      <td>7526</td>\n",
       "      <td>Le Creuset</td>\n",
       "      <td>f0188d30-aa26-8614-7a44-aa149fad66b0</td>\n",
       "      <td>2017-01-12 21:37:00</td>\n",
       "      <td>Your kitchen + our color choices = food heaven</td>\n",
       "      <td>What will you bring to the table? View in brow...</td>\n",
       "      <td>4</td>\n",
       "      <td>[kitchen, color, choices, food, heaven]</td>\n",
       "      <td>[kitchen, color, choic, food, heaven]</td>\n",
       "      <td>kitchen color choic food heaven</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6361</td>\n",
       "      <td>7526</td>\n",
       "      <td>Le Creuset</td>\n",
       "      <td>29af73ae-8956-9bdc-7148-d5eb0bde173b</td>\n",
       "      <td>2017-01-13 15:09:04</td>\n",
       "      <td>Free Shipping Starts Now + Storage Staples to ...</td>\n",
       "      <td>Plus, a sweet treat for you! LE CREUSET Cookwa...</td>\n",
       "      <td>5</td>\n",
       "      <td>[free, shipping, starts, storage, staples, get...</td>\n",
       "      <td>[free, ship, start, storag, stapl, get, organ]</td>\n",
       "      <td>free ship start storag stapl get organ</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reg_id  add_id        name                            email_guid  \\\n",
       "0    6361    7526  Le Creuset  45f2d9ed-128e-9ae5-b8f1-4e224a02dfca   \n",
       "1    6361    7526  Le Creuset  34db5cee-2a9c-17f4-b97d-68343ad26f19   \n",
       "2    6361    7526  Le Creuset  55f96ec8-739f-4a3a-63c4-ec1fddcf795d   \n",
       "3    6361    7526  Le Creuset  f0188d30-aa26-8614-7a44-aa149fad66b0   \n",
       "4    6361    7526  Le Creuset  29af73ae-8956-9bdc-7148-d5eb0bde173b   \n",
       "\n",
       "               sent_at                                            subject  \\\n",
       "0  2017-01-10 21:34:33                              Welcome, Lorem Ipsum!   \n",
       "1  2017-01-10 21:36:48     Hi! You were looking for free shipping, right?   \n",
       "2  2017-01-10 21:41:43                  Le Creuset: New Order # 200068673   \n",
       "3  2017-01-12 21:37:00     Your kitchen + our color choices = food heaven   \n",
       "4  2017-01-13 15:09:04  Free Shipping Starts Now + Storage Staples to ...   \n",
       "\n",
       "                                           full_text  r  \\\n",
       "0  LE CREUSET Welcome to Le Creuset. To log in wh...  1   \n",
       "1  Save a bundle on shipping with code LECREUSETL...  2   \n",
       "2  LE CREUSET Thank you for your order from Le Cr...  3   \n",
       "3  What will you bring to the table? View in brow...  4   \n",
       "4  Plus, a sweet treat for you! LE CREUSET Cookwa...  5   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0                            [welcome, lorem, ipsum]   \n",
       "1               [hi, looking, free, shipping, right]   \n",
       "2                         [new, order, #, 200068673]   \n",
       "3            [kitchen, color, choices, food, heaven]   \n",
       "4  [free, shipping, starts, storage, staples, get...   \n",
       "\n",
       "                                   stemmed_tokens  \\\n",
       "0                          [welcom, lorem, ipsum]   \n",
       "1                   [hi, look, free, ship, right]   \n",
       "2                      [new, order, #, 200068673]   \n",
       "3           [kitchen, color, choic, food, heaven]   \n",
       "4  [free, ship, start, storag, stapl, get, organ]   \n",
       "\n",
       "                             stemmed_text lda_predicted_label  \n",
       "0                      welcom lorem ipsum                ship  \n",
       "1                 hi look free ship right                ship  \n",
       "2                   new order # 200068673                 new  \n",
       "3         kitchen color choic food heaven                 day  \n",
       "4  free ship start storag stapl get organ                ship  "
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare against other classifiers\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans as KM \n",
    "from sklearn.cluster import AgglomerativeClustering as AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('order', 5.6068248408522319), ('thank', 0.83167091643531899), ('way', 0.41051174874127588), ('ship', 0.30714918394325963), ('receiv', 0.30191834487859665), ('shipment', 0.1698444079564331), ('10', 0.065294322800492299), ('shop', 0.051650212018372832), ('deliveri', 0.03067125141257734), ('15', 0.023221694060237316)]\n",
      "====================================================================================================\n",
      "Topic 1:\n",
      "[('ship', 3.9381823415025177), ('free', 2.5549554884881003), ('day', 0.38837109771905953), ('10', 0.21359066917282063), ('today', 0.17763187121360938), ('end', 0.14585135710459926), ('50', 0.14286364147137173), ('holiday', 0.13925627594223658), ('25', 0.13825795971340046), ('20', 0.13765613276170413)]\n",
      "====================================================================================================\n",
      "Topic 2:\n",
      "[('welcom', 4.075559663022303), ('lorem', 0.59781729156387198), ('thank', 0.2949135869388424), ('15', 0.26388705624879333), ('gift', 0.20149521065157303), ('10', 0.17781138869136051), ('account', 0.17482972344820144), ('purchas', 0.091138786861103785), ('start', 0.063453397403110484), ('20', 0.061345794427089081)]\n",
      "====================================================================================================\n",
      "Topic 3:\n",
      "[('new', 4.0610189291828602), ('account', 0.19646226447186929), ('deal', 0.11589288280076634), ('shop', 0.11488643596477977), ('style', 0.10353802666694983), ('sale', 0.05271299411760888), ('30', 0.048985920950214451), ('day', 0.042001118607750888), ('free', 0.032764453241362956), ('start', 0.031263492055024412)]\n",
      "====================================================================================================\n",
      "Topic 4:\n",
      "[('sale', 2.4702802943878277), ('save', 1.3346862213959538), ('day', 1.1932515567720345), ('extra', 1.1925575142546658), ('50', 1.16439859032672), ('end', 0.8148751180474233), ('today', 0.800312371757538), ('30', 0.70205773123460258), ('20', 0.58815045952747902), ('shop', 0.58814417060660529)]\n",
      "====================================================================================================\n",
      "Topic 5:\n",
      "[('confirm', 4.3870249456665764), ('account', 0.42367982653093561), ('shipment', 0.31261450321647494), ('order', 0.23312816062714653), ('deliveri', 0.1823095475042843), ('lorem', 0.15057165773639264), ('purchas', 0.029630383748227143), ('ship', 0.016924818485809429), ('shop', 0.0092992851367449756), ('20', 0.0010149950665926885)]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "_model = NMF(6, random_state=22)\n",
    "_model.fit(vectorized_values)\n",
    "\n",
    "labels = []\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        labels.append([(vectorizer.get_feature_names()[i]) for i in topic.argsort()[:-1-1:-1]][0])\n",
    "        print(\"=\" * 100)\n",
    "print_topics(_model, vectorizer)\n",
    "\n",
    "vectorized_values_model = _model.transform(vectorized_values)\n",
    "\n",
    "predicted_label = []\n",
    "for i in vectorized_values_model:\n",
    "    # Get the highest value. We'll consider that to be the predicted label.\n",
    "    predicted_label.append(labels[i.argsort()[-1]])\n",
    "\n",
    "df[\"nmf_predicted_label\"] = predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_model = KM(6, random_state=22)\n",
    "_model.fit(vectorized_values)\n",
    "\n",
    "labels = []\n",
    "def print_topics(model, vectorizer, top_n=3):\n",
    "    for i, t in enumerate(_model.cluster_centers_):\n",
    "        top_words = t.argsort()[:-3:-1]\n",
    "#         for w in top_words:\n",
    "#             print(vectorizer.get_feature_names()[w])\n",
    "        labels.append(vectorizer.get_feature_names()[top_words[0]])\n",
    "#         print(\"=\" * 100)\n",
    "print_topics(_model, vectorizer)\n",
    "\n",
    "vectorized_values_model = _model.transform(vectorized_values)\n",
    "\n",
    "predicted_label = []\n",
    "for i in vectorized_values_model:\n",
    "    # Get the highest value. We'll consider that to be the predicted label.\n",
    "    predicted_label.append(labels[i.argsort()[-1]])\n",
    "\n",
    "df[\"km_predicted_label\"] = predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1717\n",
      "1     206\n",
      "Name: order confirmation, dtype: int64\n",
      "0.0    1769\n",
      "1.0     154\n",
      "Name: shipping confirmation, dtype: int64\n",
      "0.0    1906\n",
      "1.0      17\n",
      "Name: delivery notification, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carl/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Load manual classification\n",
    "classified = pd.read_csv(\"./data/captsone-manually-classified.csv\")\n",
    "\n",
    "# Fill blanks with 0\n",
    "classified.fillna(0, inplace=True)\n",
    "\n",
    "# Remove any \"?\" and replace with 0\n",
    "classified[\"order confirmation\"][classified[\"order confirmation\"] == \"?\"] = 0\n",
    "\n",
    "# Print some quick summary stats\n",
    "print(classified[\"order confirmation\"].value_counts())\n",
    "print(classified[\"shipping confirmation\"].value_counts())\n",
    "print(classified[\"delivery notification\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_classification_results(row):\n",
    "    if row[\"order confirmation\"] != 0:\n",
    "        return \"order\"\n",
    "    if row[\"shipping confirmation\"] != 0:\n",
    "        return \"ship\"\n",
    "    if row[\"delivery notification\"] != 0:\n",
    "        return \"delivery\"\n",
    "    else:\n",
    "        return \"not classified\"\n",
    "\n",
    "classified[\"manual_label\"] = classified.apply(get_classification_results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "\n",
    "manual_df = classified.drop_duplicates(\"email_guid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.merge(manual_df[[\"email_guid\", \"manual_label\"]], on=\"email_guid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3346, 15)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not classified    1539\n",
       "order              205\n",
       "ship               151\n",
       "delivery            17\n",
       "Name: manual_label, dtype: int64"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"manual_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.manual_label.fillna(\"not classified\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only focus on emails classified manually\n",
    "\n",
    "indices = df[df.manual_label != \"not classified\"].index\n",
    "y_class = df.loc[indices, \"manual_label\"]\n",
    "X_class = X[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_class, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predict = lr.predict(X_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  12,   5],\n",
       "       [  0, 205,   0],\n",
       "       [  0,   9, 142]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_class, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93029490616621979"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_class, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order       205\n",
       "ship        151\n",
       "delivery     17\n",
       "Name: manual_label, dtype: int64"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only using TDIDF, this is pretty bad results\n",
    "# Let's add more data: r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_r = df.loc[indices, \"r\"]\n",
    "X_r = list(X_r)\n",
    "X_df[\"r\"] = X_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_df, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94369973190348522"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_df, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = lr.predict(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   5,   5],\n",
       "       [  1, 203,   1],\n",
       "       [  0,   9, 142]])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_class, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_df, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97050938337801607"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_df, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = rf.predict(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12,   3,   2],\n",
       "       [  0, 205,   0],\n",
       "       [  1,   5, 145]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_class, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run against topic modeling\n",
    "y = df.lda_predicted_label\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df[\"r\"] = df.r.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Split our data in test / train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2509, 33)\n",
      "(837, 33)\n",
      "(2509,)\n",
      "(837,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98365882821841366"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97491039426523296"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predict = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[315,   1,   2,   1,   2,   1],\n",
       "       [  1,  70,   0,   0,   0,   1],\n",
       "       [  0,   0, 133,   0,   1,   0],\n",
       "       [  4,   0,   1,  83,   1,   0],\n",
       "       [  0,   0,   0,   0, 144,   0],\n",
       "       [  3,   0,   0,   0,   2,  71]])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next steps:\n",
    "# - manually tweak LDA models to give more weight to specific labels\n",
    "# - consider fitting this through Keras, or some neural network to get better labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
